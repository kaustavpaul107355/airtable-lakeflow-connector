# Databricks Repos Deployment Guide (Manual Upload)

## Overview

This guide explains how to deploy the Airtable Lakeflow Connector using Databricks Repos **without Git integration**. We'll manually upload files to create a proper Python package structure.

## Why Databricks Repos?

Based on the [official Lakeflow Community Connectors repository](https://github.com/databrickslabs/lakeflow-community-connectors/tree/master/sources), connectors work properly in Repos because:

- âœ… **Proper Python package structure** - `/Repos/` provides correct module resolution
- âœ… **Custom data source serialization works** - Workers can import modules during deserialization
- âœ… **No manual `sys.path` manipulation** - Repos handles this automatically
- âœ… **Same environment as GitHub examples** - HubSpot, Salesforce, etc.

**Key Insight:** The GitHub examples don't use wheels - they use Repos directly!

## Prerequisites

1. **Unity Catalog Connection** (already created):
   ```sql
   CREATE CONNECTION airtable
   TYPE GENERIC_LAKEFLOW_CONNECT
   OPTIONS (
     sourceName 'airtable',
     bearer_token '<your_token>',
     base_id 'appSaRcgA5UCGoRg5',
     base_url 'https://api.airtable.com'
   );
   ```

2. **Databricks workspace** with Repos enabled

## Deployment Steps

### Step 1: Create Repos Folder (Manual - Via UI)

1. Go to **Repos** in Databricks sidebar
2. Click your username to expand
3. Create a new folder structure:
   - Right-click â†’ **"Create"** â†’ **"Folder"**
   - Name: `airtable-connector`
   - Full path: `/Repos/kaustav.paul@databricks.com/airtable-connector`

### Step 2: Upload Python Packages

Upload these directories to the Repos folder:

**Using Databricks CLI:**

```bash
# Navigate to your local airtable-connector directory
cd /path/to/airtable-connector

# Upload sources/
databricks workspace import-dir sources \
  /Repos/kaustav.paul@databricks.com/airtable-connector/sources \
  --profile staging

# Upload pipeline/
databricks workspace import-dir pipeline \
  /Repos/kaustav.paul@databricks.com/airtable-connector/pipeline \
  --profile staging

# Upload libs/
databricks workspace import-dir libs \
  /Repos/kaustav.paul@databricks.com/airtable-connector/libs \
  --profile staging

# Upload sdp_ingest/
databricks workspace import-dir sdp_ingest \
  /Repos/kaustav.paul@databricks.com/airtable-connector/sdp_ingest \
  --profile staging

# Upload pipeline-spec/ (optional, for validation)
databricks workspace import-dir pipeline-spec \
  /Repos/kaustav.paul@databricks.com/airtable-connector/pipeline-spec \
  --profile staging
```

**Or using deploy script:**

```bash
./deploy.sh staging repos
```

### Step 3: Verify Repos Structure

After uploading, verify in Databricks UI:

```
/Repos/kaustav.paul@databricks.com/airtable-connector/
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interface/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ lakeflow_connect.py
â”‚   â””â”€â”€ airtable/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ airtable.py
â”‚       â””â”€â”€ _generated_airtable_python_source.py
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lakeflow_python_source.py
â”‚   â””â”€â”€ ingestion_pipeline.py
â”œâ”€â”€ libs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ source_loader.py
â”œâ”€â”€ sdp_ingest/
â”‚   â””â”€â”€ airtable_sdp_repos.py
â””â”€â”€ pipeline-spec/
    â””â”€â”€ airtable_spec.py
```

**Important:** All files should be of type `FILE`, not `NOTEBOOK`. You can verify by checking file icons in the UI.

### Step 4: Create/Update DLT Pipeline

**Pipeline Configuration (JSON):**

```json
{
  "name": "Airtable Lakeflow Connector - Repos",
  "libraries": [
    {
      "notebook": {
        "path": "/Repos/kaustav.paul@databricks.com/airtable-connector/sdp_ingest/airtable_sdp_repos.py"
      }
    }
  ],
  "catalog": "kaustavpaul_demo",
  "schema": "airtable_connector",
  "continuous": false,
  "development": true,
  "photon": true,
  "channel": "CURRENT",
  "serverless": true
}
```

**Key Points:**
- âœ… Entry point: `/Repos/.../sdp_ingest/airtable_sdp_repos.py`
- âœ… No `libraries.whl` needed
- âœ… No `configuration` section needed (UC provides credentials)
- âœ… No `root_path` needed (Repos handles it)

**Via UI:**

1. Go to **Workflows** â†’ **Delta Live Tables**
2. Click **"Create Pipeline"** or edit existing
3. Settings:
   - **Name:** `Airtable Lakeflow Connector - Repos`
   - **Notebook libraries:** `/Repos/kaustav.paul@databricks.com/airtable-connector/sdp_ingest/airtable_sdp_repos.py`
   - **Target:**
     - Catalog: `kaustavpaul_demo`
     - Schema: `airtable_connector`
   - **Development mode:** Enabled
4. Save

### Step 5: Run the Pipeline

1. Go to your DLT pipeline
2. Click **"Start"**
3. Monitor execution in the pipeline UI

**Expected Output:**
- âœ… No `ModuleNotFoundError`
- âœ… Custom data source registers successfully
- âœ… UC connection `airtable` resolved automatically
- âœ… 6 bronze tables created:
  - `kaustavpaul_demo.airtable_connector.bronze_sku_candidates`
  - `kaustavpaul_demo.airtable_connector.bronze_launch_milestones`
  - `kaustavpaul_demo.airtable_connector.bronze_compliance_records`
  - `kaustavpaul_demo.airtable_connector.bronze_packaging_tasks`
  - `kaustavpaul_demo.airtable_connector.bronze_marketing_assets`
  - `kaustavpaul_demo.airtable_connector.bronze_vendors`

## How It Works

### Repos Environment Benefits

**Without Repos (/Workspace/...):**
- âŒ Modules aren't proper packages
- âŒ `sys.path` manipulation required
- âŒ Workers can't import during deserialization
- âŒ `ModuleNotFoundError: No module named 'pipeline'`

**With Repos (/Repos/...):**
- âœ… Proper Python package environment
- âœ… Modules importable on all nodes
- âœ… Custom data source serializes correctly
- âœ… **Same as GitHub examples!**

### UC Connection Resolution

```python
spark.read
  .format("lakeflow_connect")
  .option("databricks.connection", "airtable")
  .option("tableName", "SKU Candidates")
  .load()
```

**What happens:**
1. DLT sees `.option("databricks.connection", "airtable")`
2. DLT queries Unity Catalog for connection details
3. UC returns: `bearer_token`, `base_id`, `base_url`
4. Options passed to `AirtableLakeflowConnector.__init__(options)`
5. Connector fetches data from Airtable
6. Data written to Delta Lake

**No credentials in code!** âœ…

## Comparison: Workspace vs Repos

| Feature | Workspace Files | Databricks Repos |
|---------|----------------|------------------|
| Python packaging | âŒ Limited | âœ… Proper |
| Module imports | âŒ Requires sys.path | âœ… Automatic |
| Data source serialization | âŒ Fails | âœ… Works |
| Matches GitHub examples | âŒ No | âœ… Yes |
| Git integration required | N/A | âŒ No (manual upload) |
| SDP compliance | âš ï¸ Workarounds needed | âœ… Native |

## Troubleshooting

### Issue: "No such file or directory" during upload

**Cause:** Repos folder doesn't exist

**Solution:**
1. Create the parent folder in Repos UI first
2. Then upload subdirectories via CLI

### Issue: "ModuleNotFoundError" still occurs

**Cause:** Files uploaded as NOTEBOOK type instead of FILE

**Solution:**
1. Check file types in Databricks UI
2. Re-upload using `--format SOURCE` flag:
   ```bash
   databricks workspace import file.py /Repos/.../file.py \
     --language PYTHON --format SOURCE --overwrite
   ```

### Issue: "UC connection not found"

**Cause:** UC connection not accessible

**Solution:**
1. Verify: `DESCRIBE CONNECTION airtable;`
2. Check permissions on UC connection
3. Ensure pipeline has access to catalog

## Updating the Connector

When you make changes locally:

```bash
# Upload specific changed files
databricks workspace import \
  --file sources/airtable/airtable.py \
  --language PYTHON --format SOURCE --overwrite \
  /Repos/kaustav.paul@databricks.com/airtable-connector/sources/airtable/airtable.py \
  --profile staging

# Or re-upload entire directory
databricks workspace import-dir sources \
  /Repos/kaustav.paul@databricks.com/airtable-connector/sources \
  --overwrite --profile staging
```

No DLT pipeline reconfiguration needed - just restart the pipeline!

## Key Differences from Workspace Deployment

1. **Path:** `/Repos/...` instead of `/Workspace/...`
2. **Package structure:** Properly handled by Repos
3. **No `sys.path` manipulation:** Not needed in pipeline code
4. **No wheel building:** Direct file upload works

## This is the Official Pattern!

According to [databrickslabs/lakeflow-community-connectors](https://github.com/databrickslabs/lakeflow-community-connectors), official connectors:
- âœ… Use Repos structure (not wheels)
- âœ… Rely on proper Python packaging
- âœ… Work with `.option("databricks.connection")`
- âœ… No hardcoded credentials

Your connector now follows the same pattern! ğŸ‰

## References

- [Lakeflow Community Connectors](https://github.com/databrickslabs/lakeflow-community-connectors)
- [Databricks Repos Documentation](https://docs.databricks.com/repos/index.html)
- [Delta Live Tables Documentation](https://docs.databricks.com/delta-live-tables/index.html)
- [Unity Catalog Connections](https://docs.databricks.com/data-governance/unity-catalog/connections.html)

