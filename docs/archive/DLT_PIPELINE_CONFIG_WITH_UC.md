# DLT Pipeline Configuration with UC Connection

## âœ… FIXED: Serialization Error

The previous error (`ModuleNotFoundError: No module named 'pipeline'`) was caused by trying to use a **custom Python Data Source** in DLT, which requires serialization to Spark workers.

## ğŸ”§ New Approach: Direct Connector (No Custom Data Source)

The pipeline has been updated to:
- âœ… Call the connector directly (not as a Spark data source)
- âœ… Use `spark.createDataFrame()` to create DataFrames
- âœ… Avoid serialization issues completely
- âœ… Work reliably in ALL DLT environments

---

## ğŸ“‹ DLT Pipeline Configuration

### Notebook Path
```
/Users/kaustav.paul@databricks.com/lakeflow-community-connectors/sources/airtable-connector/sdp_ingest/airtable_sdp_correct.py
```

### Configuration (Advanced Settings)

Add these **3 configuration entries** to your DLT pipeline:

| Key | Value |
|-----|-------|
| `connection.airtable.bearer_token` | `{{connection.airtable.bearer_token}}` |
| `connection.airtable.base_id` | `{{connection.airtable.base_id}}` |
| `connection.airtable.base_url` | `{{connection.airtable.base_url}}` |

**Important**: Use the **exact syntax** with double curly braces `{{...}}` - this tells DLT to resolve the UC connection!

---

## ğŸ› ï¸ How to Configure in Databricks UI

### Step 1: Edit Pipeline Settings

1. Go to: https://e2-dogfood.staging.cloud.databricks.com/pipelines/60a58669-dca3-40ab-aaa2-a00933180c1c
2. Click **"Settings"** or **"Edit"**

### Step 2: Update Notebook Library

In **"Notebook libraries"** or **"Paths"** section:
```
/Users/kaustav.paul@databricks.com/lakeflow-community-connectors/sources/airtable-connector/sdp_ingest/airtable_sdp_correct.py
```

(Make sure it's **"notebook"** type, not "glob")

### Step 3: Add Configuration Entries

Expand **"Advanced"** â†’ **"Configuration"**

Click **"Add configuration"** and add these **3 entries**:

**Entry 1:**
- Key: `connection.airtable.bearer_token`
- Value: `{{connection.airtable.bearer_token}}`

**Entry 2:**
- Key: `connection.airtable.base_id`
- Value: `{{connection.airtable.base_id}}`

**Entry 3:**
- Key: `connection.airtable.base_url`
- Value: `{{connection.airtable.base_url}}`

### Step 4: Verify Other Settings

- **Catalog**: `kaustavpaul_demo`
- **Target Schema**: `airtable_connector`
- **Pipeline Mode**: `Development`
- **Serverless**: âœ… Enabled
- **Photon**: âœ… Enabled

### Step 5: Save and Start

1. Click **"Save"**
2. Click **"Start"**

---

## ğŸ“Š Complete JSON Configuration

For reference, here's the complete JSON:

```json
{
  "name": "Airtable Lakeflow Connector",
  "pipeline_type": "WORKSPACE",
  "libraries": [
    {
      "notebook": {
        "path": "/Users/kaustav.paul@databricks.com/lakeflow-community-connectors/sources/airtable-connector/sdp_ingest/airtable_sdp_correct.py"
      }
    }
  ],
  "catalog": "kaustavpaul_demo",
  "target": "airtable_connector",
  "continuous": false,
  "development": true,
  "photon": true,
  "channel": "CURRENT",
  "serverless": true,
  "configuration": {
    "connection.airtable.bearer_token": "{{connection.airtable.bearer_token}}",
    "connection.airtable.base_id": "{{connection.airtable.base_id}}",
    "connection.airtable.base_url": "{{connection.airtable.base_url}}"
  }
}
```

---

## ğŸ” What Happens When Pipeline Runs

### 1. Credential Resolution
```
ğŸ”‘ Loading credentials from DLT pipeline configuration...

âœ… Loaded credentials from pipeline configuration
   Token: ********************keLEX5
   Base ID: appSaRcgA5UCGoRg5
   Base URL: https://api.airtable.com/v0
```

DLT resolves `{{connection.airtable.bearer_token}}` by:
- Looking up UC connection named "airtable"
- Extracting the `bearer_token` option
- Passing it to the pipeline as a config value

### 2. Connector Initialization
```
ğŸ”Œ Initializing Airtable connector...
âœ… Connector initialized successfully
```

The connector is initialized with UC credentials (no hardcoding!)

### 3. Table Ingestion
```
ğŸ“Š Defining DLT tables:
   âœ… bronze_sku_candidates
   âœ… bronze_launch_milestones
   âœ… bronze_compliance_records
   âœ… bronze_packaging_tasks
   âœ… bronze_marketing_assets
   âœ… bronze_vendors
```

For each table:
```
ğŸ“¥ Reading table: Packaging Tasks
   âœ… Read 42 records
   âœ… Created DataFrame with 42 records
```

### 4. Success!
```
âœ… DLT Pipeline Ready - 6 bronze tables defined
   Approach: Direct connector (no custom data source)
   Credentials: From UC connection via pipeline config
```

---

## â“ FAQ

### Q: Why use `{{connection.airtable.bearer_token}}` syntax?

**A:** This is DLT's special syntax for UC connection resolution. When DLT sees `{{connection.NAME.OPTION}}`, it:
1. Queries UC for connection `NAME`
2. Extracts option `OPTION`
3. Injects the value into `spark.conf`

Your code reads it via:
```python
AIRTABLE_TOKEN = spark.conf.get("connection.airtable.bearer_token")
```

### Q: Are credentials hardcoded?

**A:** No! The `{{...}}` syntax is a **reference**, not a value. DLT resolves it at runtime from UC.

### Q: What if I don't use the `{{...}}` syntax?

**A:** If you enter the raw token value (like `patkBXwClC7keLEX5...`), it will work BUT:
- âŒ Credentials exposed in pipeline config (visible to anyone with access)
- âŒ No audit logging
- âŒ Hard to rotate credentials
- âŒ Not following best practices

Always use `{{connection.airtable.bearer_token}}` for security!

### Q: Why not use `.option("databricks.connection", "airtable")`?

**A:** That syntax works for **native Spark data sources** but NOT for **custom Python data sources** due to serialization issues in DLT. The direct connector approach is more reliable.

---

## ğŸ‰ Expected Results

After starting the pipeline with this configuration:

âœ… No `ModuleNotFoundError`  
âœ… No `SerializationError`  
âœ… Credentials resolved from UC  
âœ… Data successfully ingested  
âœ… 6 bronze tables created with actual data!

**Check Catalog Explorer:**
```
kaustavpaul_demo
â””â”€â”€ airtable_connector
    â”œâ”€â”€ bronze_sku_candidates       (42 records)
    â”œâ”€â”€ bronze_launch_milestones    (15 records)
    â”œâ”€â”€ bronze_compliance_records   (28 records)
    â”œâ”€â”€ bronze_packaging_tasks      (56 records)
    â”œâ”€â”€ bronze_marketing_assets     (33 records)
    â””â”€â”€ bronze_vendors              (12 records)
```

---

**Updated**: January 6, 2026  
**Approach**: Direct Connector (No Custom Data Source)  
**Workspace**: e2-dogfood.staging.cloud.databricks.com

