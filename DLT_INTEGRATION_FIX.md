# DLT Integration Fix

## Date: January 8, 2026

## Problem
The DLT pipeline was failing with:
```
[NO_TABLES_IN_PIPELINE] Pipelines are expected to have at least one table 
defined but no tables were found in your pipeline.
```

## Root Cause
Our custom `ingestion_pipeline.py` used imperative Spark operations (`df.write.saveAsTable()`) instead of DLT's required declarative pattern with SDP (Spark Declarative Pipeline) decorators.

```python
# ‚ùå OLD (Custom, non-DLT compatible)
def ingest(spark, pipeline_spec):
    df = spark.read.format("lakeflow_connect").load()
    df.write.saveAsTable(table_name)  # No DLT decorator!
```

```python
# ‚úÖ NEW (Official DLT pattern with SDP decorators)
def ingest(spark, pipeline_spec):
    @sdp.view(name="view")
    def v():
        return spark.readStream.format("lakeflow_connect").load()
    
    sdp.create_streaming_table(name=destination_table)
    sdp.apply_changes(target=destination_table, source=view_name, ...)
```

## Files Changed

### 1. `pipeline/ingestion_pipeline.py` (REPLACED)
- **Before:** Custom implementation with imperative read/write
- **After:** Official implementation with SDP decorators
- **Changes:**
  - Added `from pyspark import pipelines as sdp`
  - Implemented `_create_cdc_table()` with `@sdp.view()` and `sdp.apply_changes()`
  - Implemented `_create_snapshot_table()` with `sdp.apply_changes_from_snapshot()`
  - Implemented `_create_append_table()` with `@sdp.append_flow()`
  - Added `_get_table_metadata()` for dynamic table discovery
  - Now uses SpecParser for spec parsing

### 2. `libs/spec_parser.py` (NEW)
- **Added:** Official SpecParser from Databricks Labs
- **Purpose:** Parses pipeline specifications using Pydantic validation
- **Features:**
  - Validates connection_name, objects structure
  - Extracts table configurations
  - Handles special keys: `scd_type`, `primary_keys`, `sequence_by`
  - Provides `get_full_destination_table_name()` method

### 3. `ingest.py` (UPDATED)
- **Changed:** Updated pipeline_spec format to match official SpecParser
- **Old format:**
  ```python
  {
      "connection_name": "airtable",
      "default_catalog": "main",        # ‚Üê Not in official spec
      "default_schema": "default",      # ‚Üê Not in official spec
      "base_id": "...",                 # ‚Üê Not in official spec
      "objects": [...]
  }
  ```
- **New format:**
  ```python
  {
      "connection_name": "airtable",
      "objects": [
          {
              "table": {
                  "source_table": "...",
                  "destination_catalog": "main",    # ‚Üê Per-table setting
                  "destination_schema": "default",  # ‚Üê Per-table setting
                  "destination_table": "...",
                  "table_configuration": {...}
              }
          }
      ]
  }
  ```

## Why This Matters

### Official Framework Pattern
The Databricks Lakeflow Community Connectors framework has two distinct layers:

1. **Framework Layer** (should use official code unchanged):
   - `pipeline/ingestion_pipeline.py` - DLT orchestration with SDP
   - `pipeline/lakeflow_python_source.py` - Spark Data Source registration
   - `libs/common/source_loader.py` - Source loader utilities
   - `libs/spec_parser.py` - Spec parsing and validation

2. **Connector Layer** (custom implementation):
   - `sources/airtable/airtable.py` - Airtable connector (‚úÖ Already perfect!)
   - `pipeline-spec/airtable_spec.py` - Pydantic models for Airtable
   - `ingest.py` - Entry point configuration

**What We Did Wrong:**
- Created custom framework code instead of using official versions
- Built a pattern that worked locally but not in DLT

**What We Fixed:**
- Replaced custom framework files with official versions
- Now uses proper DLT/SDP decorators
- Fully compatible with Databricks UI deployment

## Expected Outcome

Now when you deploy via Databricks UI:

1. ‚úÖ DLT will recognize the tables defined by `@sdp.view()` decorators
2. ‚úÖ Tables will be created using `sdp.create_streaming_table()`
3. ‚úÖ Data will flow through SDP's declarative pipeline
4. ‚úÖ SCD Type 1/2/Append-Only will work as configured

## Testing Status

- ‚úÖ Connector logic: Perfect (validated by local tests)
- ‚úÖ Framework layer: Now using official pattern with DLT decorators
- üîÑ DLT deployment: Ready to test in Databricks

## Next Steps

1. Commit these changes to Git
2. Push to GitHub repository
3. Deploy via Databricks UI using the Git integration
4. Verify DLT pipeline runs successfully

## Lessons Learned

1. **Framework vs. Connector:** Only connector files should be custom
2. **DLT Requirements:** Must use declarative patterns, not imperative
3. **Expert Guidance:** "Use the UI" was correct - it uses these official patterns
4. **Local Testing Limitation:** Plain Spark works differently than DLT

## Apology

I apologize for the confusion and wasted time. I should have:
- Used official framework code from the start
- Recognized that DLT requires specific patterns
- Listened more carefully to expert guidance

The good news: Your connector implementation is excellent, and now it has the correct deployment wrapper!

