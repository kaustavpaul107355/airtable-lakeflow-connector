# Airtable Lakeflow Community Connector

A production-ready Databricks Lakeflow connector for ingesting data from Airtable into Delta tables using Unity Catalog connections.

**Status:** ‚úÖ Implementation Complete - Ready for Official Tool Integration  
**Framework:** [Databricks Lakeflow Community Connectors](https://github.com/databrickslabs/lakeflow-community-connectors)  
**Last Updated:** January 8, 2026

---

## ‚ö†Ô∏è **IMPORTANT: Two Deployment Modes**

This connector supports both **local testing** and **Databricks deployment**:

| File | Purpose | Where to Use |
|------|---------|--------------|
| **`ingest.py`** | Databricks deployment (production) | ‚òÅÔ∏è Databricks workspace |
| **`ingest_local.py`** | Local testing with mock Spark | üíª Your local machine |

**Use `ingest.py` for all Databricks deployments** - it has the correct import paths and no `__file__` dependencies.

üìö **Documentation:**
- **[Databricks Deployment Guide](./docs/DEPLOYMENT.md)** - Complete Databricks deployment instructions
- **[Local Testing Guide](./docs/LOCAL_TESTING.md)** - Local development and testing

---

## üéØ Quick Start

This connector is designed to be deployed using the official Databricks UI or CLI tools.

### Using Databricks UI (Recommended)
1. Go to Databricks workspace
2. Click **"+New"** ‚Üí **"Add or upload data"** ‚Üí **"Community connectors"**
3. Click **"+ Add Community Connector"**
4. Point to this repository
5. Configure tables and destination
6. Deploy!

### Using CLI Tool
```bash
# Clone the official repository
git clone https://github.com/databrickslabs/lakeflow-community-connectors.git
cd lakeflow-community-connectors/tools/community_connector

# Use CLI to create connector (follow tool documentation)
# Integrate this Airtable connector code
```

---

## üìä What This Connector Does

### Capabilities
- ‚úÖ **Full Airtable Integration** - Connect to any Airtable base
- ‚úÖ **Unity Catalog Support** - Secure credential management via UC connections
- ‚úÖ **Incremental Reads** - Efficient data synchronization
- ‚úÖ **Schema Detection** - Automatic schema discovery
- ‚úÖ **Type Mapping** - Airtable types ‚Üí Spark types
- ‚úÖ **Multiple Tables** - Sync multiple tables simultaneously
- ‚úÖ **SCD Type 2** - Historical tracking support

### Supported Airtable Features
- **Tables:** Any table in your Airtable base
- **Fields:** All standard field types (text, number, date, attachments, etc.)
- **Formulas:** Read formula values
- **Linked Records:** Capture linked record IDs
- **Attachments:** Store attachment metadata and URLs

---

## üèóÔ∏è Project Structure

```
airtable-connector/
‚îú‚îÄ‚îÄ sources/                           # Connector implementation
‚îÇ   ‚îú‚îÄ‚îÄ airtable/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ airtable.py               # ‚úÖ Main connector (production-ready)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # Connector-specific docs
‚îÇ   ‚îî‚îÄ‚îÄ interface/
‚îÇ       ‚îú‚îÄ‚îÄ lakeflow_connect.py        # Base interface
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ pipeline-spec/                     # Pipeline specification
‚îÇ   ‚îú‚îÄ‚îÄ airtable_spec.py              # ‚úÖ Pydantic spec (production-ready)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ pipeline/                          # Framework files
‚îÇ   ‚îú‚îÄ‚îÄ ingestion_pipeline.py         # Core ingestion logic
‚îÇ   ‚îú‚îÄ‚îÄ lakeflow_python_source.py     # PySpark Data Source
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ libs/                              # Shared utilities
‚îÇ   ‚îî‚îÄ‚îÄ common/
‚îÇ       ‚îú‚îÄ‚îÄ source_loader.py          # Module loading
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ tests/                             # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_airtable_connector.py    # Connector tests
‚îÇ   ‚îú‚îÄ‚îÄ test_pipeline_spec.py         # Spec tests
‚îÇ   ‚îú‚îÄ‚îÄ test_pydantic_integration.py  # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                   # Fixtures
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ archive/                       # Historical/learning materials
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ OFFICIAL_APPROACH_GUIDE.md        # Deployment guide
‚îî‚îÄ‚îÄ CLEANUP_REPORT.md                 # Cleanup documentation
```

---

## üîë Prerequisites

### 1. Unity Catalog Connection

Create a UC connection for Airtable:

```sql
CREATE CONNECTION airtable
TYPE GENERIC_LAKEFLOW_CONNECT
OPTIONS (
  base_url 'https://api.airtable.com',
  base_id 'your_base_id',
  access_token 'your_access_token'
);
```

**How to get credentials:**
- **Base ID:** Found in Airtable URL: `https://airtable.com/{base_id}/...`
- **Access Token:** Create at https://airtable.com/create/tokens
  - Required scopes: `data.records:read`, `schema.bases:read`

### 2. Databricks Requirements
- Unity Catalog enabled
- Delta Live Tables (DLT) access
- Workspace permissions for creating connectors

---

## üìã Configuration Example

When using the UI or CLI tools, configure your connector with a pipeline spec:

```python
pipeline_spec = {
    "connection_name": "airtable",  # UC connection name
    "base_id": "appXXXXXXXXXXXXXX",
    "default_catalog": "my_catalog",
    "default_schema": "airtable_data",
    "objects": [
        {
            "table": {
                "source_table": "Tasks",
                "destination_table": "tasks",
                "primary_keys": ["id"]
            }
        },
        {
            "table": {
                "source_table": "Projects",
                "destination_table": "projects",
                "primary_keys": ["id"]
            }
        }
    ]
}
```

---

## üöÄ Implementation Details

### Connector Class: `AirtableLakeflowConnector`

Located in `sources/airtable/airtable.py`:

```python
class AirtableLakeflowConnector(LakeflowConnect):
    """
    Airtable connector implementing the LakeflowConnect interface.
    
    Supports:
    - Dynamic schema discovery
    - Incremental data reads
    - UC connection credential resolution
    - Type mapping (Airtable ‚Üí Spark)
    """
    
    def __init__(self, options: dict[str, str]) -> None:
        """Initialize with UC connection options."""
    
    def list_tables(self) -> list[str]:
        """List all tables in the Airtable base."""
    
    def get_table_schema(self, table_name: str, ...) -> StructType:
        """Get Spark schema for an Airtable table."""
    
    def read_table_metadata(self, table_name: str, ...) -> dict:
        """Get table metadata (keys, cursor field, ingestion type)."""
    
    def read_table(self, table_name: str, ...) -> (Iterator[dict], dict):
        """Read table data incrementally."""
```

### Pipeline Specification: `AirtablePipelineSpec`

Located in `pipeline-spec/airtable_spec.py`:

```python
class AirtablePipelineSpec(BaseModel):
    """
    Pydantic model for pipeline configuration validation.
    
    Features:
    - Field validation
    - Type checking
    - Default value handling
    - Pydantic v2 compatible
    """
    
    connection_name: str  # UC connection
    base_id: Optional[str]  # Airtable base ID
    default_catalog: str  # Target catalog
    default_schema: str  # Target schema
    objects: List[TableSpec]  # Tables to sync
```

---

## üß™ Testing

Run the test suite:

```bash
# Install dependencies
pip install pytest pydantic pyairtable pyspark

# Run all tests
pytest tests/

# Run specific test
pytest tests/test_airtable_connector.py -v

# Run with coverage
pytest tests/ --cov=sources --cov=pipeline-spec
```

### Test Coverage:
- ‚úÖ Connector initialization and authentication
- ‚úÖ Table listing and schema detection
- ‚úÖ Type mapping validation
- ‚úÖ Incremental read logic
- ‚úÖ Pipeline spec validation (Pydantic v2)
- ‚úÖ UC connection integration

---

## üìö Documentation

### Main Documentation:
- **[OFFICIAL_APPROACH_GUIDE.md](./OFFICIAL_APPROACH_GUIDE.md)** - Deployment using UI/CLI tools
- **[sources/airtable/README.md](./sources/airtable/README.md)** - Connector-specific documentation
- **[CLEANUP_REPORT.md](./CLEANUP_REPORT.md)** - Codebase organization details

### Archived Learning Materials:
See `docs/archive/` for historical documentation and troubleshooting guides created during development.

---

## üîß Technical Details

### Authentication
- Uses Unity Catalog connections (`GENERIC_LAKEFLOW_CONNECT`)
- No credentials in code or configuration
- Secure token management via UC

### Data Flow
```
Airtable API
    ‚Üì
UC Connection (credentials)
    ‚Üì
AirtableLakeflowConnector (this code)
    ‚Üì
Spark Data Source API
    ‚Üì
Delta Live Tables
    ‚Üì
Delta Tables (Unity Catalog)
```

### Type Mapping

| Airtable Type | Spark Type | Notes |
|---------------|------------|-------|
| singleLineText | StringType | - |
| multilineText | StringType | - |
| number | DoubleType | Includes decimals |
| currency | DecimalType(18,2) | Fixed precision |
| date | DateType | - |
| dateTime | TimestampType | With timezone |
| checkbox | BooleanType | - |
| singleSelect | StringType | Value stored |
| multipleSelects | ArrayType(StringType) | Array of values |
| multipleRecordLinks | ArrayType(StringType) | Array of linked IDs |
| attachment | ArrayType(StructType) | Array of attachment objects |
| formula | StringType | Computed value |
| rollup | StringType | Aggregated value |

---

## üêõ Troubleshooting

### Common Issues:

**Issue:** "Connection 'airtable' not found"  
**Solution:** Create UC connection (see Prerequisites section)

**Issue:** "Invalid credentials"  
**Solution:** Verify access token has required scopes and is valid

**Issue:** "Base not found"  
**Solution:** Check base_id in UC connection matches your Airtable base

**Issue:** "Table not found"  
**Solution:** Verify table name matches exactly (case-sensitive)

For more help, see the [GitHub repository issues](https://github.com/databrickslabs/lakeflow-community-connectors/issues).

---

## ü§ù Contributing

This connector follows the [Lakeflow Community Connectors](https://github.com/databrickslabs/lakeflow-community-connectors) framework.

To contribute:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

See `CONTRIBUTING.md` in the main repository for detailed guidelines.

---

## üìÑ License

This project follows the license of the parent [Lakeflow Community Connectors](https://github.com/databrickslabs/lakeflow-community-connectors) repository.

---

## üôè Acknowledgments

- Built on the [Databricks Lakeflow Community Connectors](https://github.com/databrickslabs/lakeflow-community-connectors) framework
- Uses the [Spark Python Data Source API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataSource.html)
- Integrates with [Airtable API](https://airtable.com/developers/web/api/introduction)

---

## üìû Support

- **Framework Issues:** [GitHub Issues](https://github.com/databrickslabs/lakeflow-community-connectors/issues)
- **Airtable API:** [Airtable Support](https://support.airtable.com/)
- **Databricks:** [Databricks Documentation](https://docs.databricks.com/)

---

## üöÄ Next Steps

1. **Review** [OFFICIAL_APPROACH_GUIDE.md](./OFFICIAL_APPROACH_GUIDE.md)
2. **Set up** Unity Catalog connection
3. **Deploy** using Databricks UI or CLI tool
4. **Configure** your tables and run the pipeline
5. **Monitor** data ingestion in DLT

**Your connector is ready to deploy!** ‚ú®
